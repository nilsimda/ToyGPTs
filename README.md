# ToyGPTs
Small GPTs trained for all sorts of different purposes. Currently these include:
* Classic Author GPTs that can be trained to mimick text from different iconic authors such as Shakespear or Dickens.
* A Calculator GPT that can do basic mathematical operations (add, sub, mul, div) by simply predicting the next token.
* A Compression GPT that can compress arbitrary text via arithmetic coding on the probability output.
